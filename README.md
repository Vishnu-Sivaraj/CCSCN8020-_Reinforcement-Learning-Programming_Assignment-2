# ğŸš• CSCN8020 â€“ Reinforcement Learning Programming  
## Assignment 2 â€“ Q-Learning Parameter Analysis (Taxi-v3)

---

## ğŸ‘¨â€ğŸ“ Student Information
**Student:** Vishnu Sivaraj  
**Course:** CSCN8020 â€“ Reinforcement Learning Programming  
**Instructor:** Prof. David Espinosa Carrillo  
**Institution:** Conestoga College  
**Term:** Winter 2026  

---

## ğŸ“Œ Project Overview

This project analyzes the performance of the **Q-Learning Reinforcement Learning algorithm** using the **Taxi-v3 environment** from Gymnasium.

The objective is to understand how different **learning rates (Î±)** and **exploration rates (Îµ)** influence:

- Learning speed
- Convergence stability
- Policy efficiency
- Agent performance

Multiple experiments were conducted and evaluated using quantitative metrics and visualization plots.

---

## ğŸ§  Environment Description

Taxi-v3 simulates a taxi agent operating in a grid world.

### Task
1. Navigate environment
2. Pick up passenger
3. Drop passenger at destination

### Environment Properties
- **States:** 500
- **Actions:** 6
- **Rewards**
  - +20 â†’ Successful drop-off
  - âˆ’1 â†’ Each movement step
  - âˆ’10 â†’ Illegal pickup/dropoff

---

## âš™ï¸ Algorithm

### Q-Learning (Off-Policy TD Control)

Update rule:

\[
Q(s,a) = Q(s,a) + \alpha [r + \gamma \max_a Q(s',a) - Q(s,a)]
\]

Where:

- Î± â†’ Learning Rate  
- Î³ â†’ Discount Factor  
- Îµ â†’ Exploration Rate (Îµ-greedy)

---

## ğŸ§ª Experiments Conducted

### Learning Rate Experiments
- Î± = 0.001
- Î± = 0.01
- Î± = 0.1
- Î± = 0.2

### Exploration Experiments
- Îµ = 0.2
- Îµ = 0.3

Training configuration:

- Episodes: **5000**
- Discount factor: **Î³ = 0.9**

---

## ğŸ“Š Evaluation Metrics

The agent performance was evaluated using:

- Average Return
- Average Return (Last 1000 Episodes)
- Evaluation Reward
- Average Steps
- Success Rate
- Training Time

---

## ğŸ† Best Hyperparameter Combination
Î± = 0.2
Îµ = 0.1
Î³ = 0.9


### Why this works best
âœ… Fast convergence  
âœ… Highest evaluation reward  
âœ… Lowest number of steps  
âœ… Stable learning behaviour  
âœ… Near 100% success rate  

---

## ğŸ“ˆ Results Visualization

Learning curves for all experiments are available in:
plots/


They demonstrate:

- Small learning rate â†’ slow learning
- High learning rate â†’ faster convergence
- Balanced exploration â†’ optimal performance

---

## ğŸ“‚ Repository Structure

RL_Assignment2/
â”‚
â”œâ”€â”€ plots/ # Training graphs
â”œâ”€â”€ logs/ # Experiment logs
â”‚
â”œâ”€â”€ assignment2_utils.py # Instructor provided utilities
â”œâ”€â”€ qlearning_taxi_fixed.py # Main training script
â”œâ”€â”€ complete_results_fixed.csv # Experiment results
â”œâ”€â”€ Assignment2_Report.pdf # Final report
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ›  Installation

Install dependencies:

```bash

pip install -r requirements.txt

Required packages:

numpy

gymnasium

matplotlib

pandas

------

â–¶ï¸ Run Experiments

python qlearning_taxi_fixed.py

The script will:

âœ… Train agent
âœ… Run all experiments
âœ… Generate plots
âœ… Save logs
âœ… Export results table

-----

ğŸ§  Learning Outcomes

This assignment demonstrates:

Reinforcement Learning fundamentals

Exploration vs Exploitation trade-off

Hyperparameter tuning

Experimental evaluation of RL agents

----

ğŸ“„ License

Academic use only â€” Conestoga College Coursework.