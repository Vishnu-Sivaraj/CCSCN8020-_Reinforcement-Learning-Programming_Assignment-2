CSCN8020 â€” Assignment 2
Q-Learning Parameter Analysis (Taxi-v3 Environment)
ğŸ‘¨â€ğŸ“ Student Information

Student Name: Vishnu Sivaraj
Course: CSCN8020 â€“ Reinforcement Learning
Instructor: Prof. David Espinosa Carrillo
Institution: Conestoga College
Term: Winter 2026

ğŸ“Œ Project Overview

This assignment investigates the performance of the Q-Learning Reinforcement Learning algorithm using the Taxi-v3 environment from Gymnasium.

The goal is to analyze how different learning rates (Î±) and exploration rates (Îµ) affect:

Learning speed

Policy stability

Agent efficiency

Convergence behaviour

Multiple experiments were conducted, evaluated, and compared using performance metrics and visualization plots.

ğŸ§  Environment Description

The Taxi-v3 environment simulates a taxi agent that must:

Navigate a grid world

Pick up a passenger

Deliver the passenger to the correct destination

Environment Properties

State Space: 500 states

Action Space: 6 actions

Reward System

+20 â†’ Successful drop-off

âˆ’1 â†’ Step penalty

âˆ’10 â†’ Illegal pickup/dropoff

âš™ï¸ Algorithm Used
Q-Learning

Q-Learning is an off-policy Temporal Difference learning algorithm.

Update rule:

ğ‘„
(
ğ‘ 
,
ğ‘
)
=
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
]
Q(s,a)=Q(s,a)+Î±[r+Î³
a
max
	â€‹

Q(s
â€²
,a)âˆ’Q(s,a)]

Where:

Î± â†’ Learning rate

Î³ â†’ Discount factor

Îµ â†’ Exploration rate (Îµ-greedy policy)

ğŸ§ª Experiments Performed
Learning Rate Experiments

Î± = 0.001

Î± = 0.01

Î± = 0.1

Î± = 0.2

Exploration Experiments

Îµ = 0.2

Îµ = 0.3

Training settings:

Episodes: 5000

Discount factor: Î³ = 0.9

ğŸ“Š Evaluation Metrics

The following metrics were used:

Average Return

Average Return (Last 1000 Episodes)

Evaluation Reward

Average Steps per Episode

Success Rate

Training Time

ğŸ† Best Hyperparameter Combination

Based on experimental results:

Learning Rate (Î±) = 0.2
Exploration Rate (Îµ) = 0.1
Discount Factor (Î³) = 0.9
Why this works best:

Fast convergence

Highest evaluation reward

Lowest average steps

Stable learning behaviour

Near 100% success rate

ğŸ“ˆ Results

Plots showing learning performance are available inside the plots/ folder.

They demonstrate:

Small Î± â†’ slow learning

Large Î± â†’ faster convergence

Moderate Îµ â†’ balanced exploration and exploitation

ğŸ“‚ Project Structure
RL_Assignment2/
â”‚
â”œâ”€â”€ logs/                     # Training logs
â”œâ”€â”€ plots/                    # Learning curves
â”‚
â”œâ”€â”€ assignment2_utils.py      # Provided utility file
â”œâ”€â”€ qlearning_taxi_fixed.py   # Main training script
â”œâ”€â”€ complete_results_fixed.csv
â”œâ”€â”€ Assignment2_Report.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
ğŸ› ï¸ Installation

Install required packages:

pip install -r requirements.txt

Dependencies:

numpy

gymnasium

matplotlib

pandas

â–¶ï¸ How to Run

Run all experiments:

python qlearning_taxi_fixed.py

The script will automatically:

âœ… Train the agent
âœ… Generate plots
âœ… Save logs
âœ… Export results table

ğŸ§  Learning Outcomes

This project demonstrates:

Reinforcement learning fundamentals

Exploration vs exploitation trade-off

Hyperparameter tuning

Performance evaluation of RL agents

ğŸ“„ License

Academic use only â€” Conestoga College coursework.